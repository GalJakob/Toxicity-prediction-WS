{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GalJakob/Toxicity-prediction-WS/blob/main/SMILES_MFP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3ijoc6OvJjVl"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "\n",
        "dataset_name = \"cardio\" # change to cardio / tox21 / clintox\n",
        "is_train_aug = 1 # 0 = non-augmented, 1 = augmented\n",
        "\n",
        "\n",
        "if is_train_aug == 1:\n",
        "  ds_train_aug = dataset_name + \"_train_aug\"\n",
        "  ds_test = dataset_name + \"_test\"\n",
        "  path_train = f\"https://raw.githubusercontent.com/GalJakob/Toxicity-prediction-WS/main/datasets/train%20datasets/{ds_train_aug}.csv\"\n",
        "  path_test = f\"https://raw.githubusercontent.com/GalJakob/Toxicity-prediction-WS/main/datasets/test%20datasets/{ds_test}.csv\"\n",
        "\n",
        "  try: #getting data from github\n",
        "    test_data = pd.read_csv(path_test)\n",
        "    train_data = pd.read_csv(path_train)\n",
        "\n",
        "  except: #uploading data instead from github\n",
        "    data = files.upload()\n",
        "    data1 = io.BytesIO(data[ds_train_aug])\n",
        "    data2 = io.BytesIO(data[ds_test])\n",
        "    train_data = pd.read_csv(data1)\n",
        "    test_data = pd.read_csv(data2)\n",
        "\n",
        "else:\n",
        "  ds_train = dataset_name + \"_train\"\n",
        "  ds_test = dataset_name + \"_test\"\n",
        "  path_train = f\"https://raw.githubusercontent.com/GalJakob/Toxicity-prediction-WS/main/datasets/train%20datasets/{ds_train}.csv\"\n",
        "  path_test = f\"https://raw.githubusercontent.com/GalJakob/Toxicity-prediction-WS/main/datasets/test%20datasets/{ds_test}.csv\"\n",
        "  try: #getting data from github\n",
        "    test_data = pd.read_csv(path_test)\n",
        "    train_data = pd.read_csv(path_train)\n",
        "\n",
        "  except: #uploading data instead from github\n",
        "    data = files.upload()\n",
        "    data1 = io.BytesIO(data[ds_train])\n",
        "    data2 = io.BytesIO(data[ds_test])\n",
        "    train_data = pd.read_csv(data1)\n",
        "    test_data = pd.read_csv(data2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rdkit"
      ],
      "metadata": {
        "id": "T4RFAVNWJmNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import callbacks\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.losses import mean_absolute_error\n",
        "from tensorflow.keras.layers import Dense, Input, Activation\n",
        "from tensorflow.keras.layers import BatchNormalization, Add, Dropout\n",
        "from tensorflow.python.keras.layers.advanced_activations import LeakyReLU\n",
        "from tensorflow.keras.optimizers import Adam, Adadelta, SGD\n",
        "\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, average_precision_score\n",
        "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "tf.keras.utils.set_random_seed(42)"
      ],
      "metadata": {
        "id": "e2MtonyVJmLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing\n",
        "def preprocess_smiles(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return None\n",
        "    return mol\n",
        "\n",
        "train_data['mol'] = train_data['smiles'].apply(preprocess_smiles).dropna()\n",
        "test_data['mol'] = test_data['smiles'].apply(preprocess_smiles).dropna()\n",
        "\n",
        "def generate_fingerprint(mol):\n",
        "    if dataset_name == 'cardio':\n",
        "        fingerprint = AllChem.GetMorganFingerprintAsBitVect(mol, radius=1, nBits=1024, useFeatures=True, useChirality=True)\n",
        "    elif dataset_name == 'clintox':\n",
        "        fingerprint = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=2048, useFeatures=True, useChirality=True)\n",
        "    else: # tox21\n",
        "        fingerprint = AllChem.GetMorganFingerprintAsBitVect(mol, radius=1, nBits=2048, useFeatures=True, useChirality=True)\n",
        "\n",
        "    return fingerprint\n",
        "\n",
        "\n",
        "train_data['fingerprint'] = train_data['mol'].apply(generate_fingerprint)\n",
        "test_data['fingerprint'] = test_data['mol'].apply(generate_fingerprint)"
      ],
      "metadata": {
        "id": "vi9hL_ZFMWq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting Data\n",
        "X_train = np.array(train_data['fingerprint'].tolist())\n",
        "y_train = np.array(train_data['label'])\n",
        "X_test = np.array(test_data['fingerprint'].tolist())\n",
        "y_test = np.array(test_data['label'])\n",
        "\n",
        "length = X_train.shape[1]\n",
        "\n",
        "# Calculate class weights\n",
        "from sklearn.utils import compute_class_weight\n",
        "train_l = train_data['label']\n",
        "cw = compute_class_weight(\n",
        "    class_weight = \"balanced\",\n",
        "    classes = np.unique(train_l),\n",
        "    y = train_l\n",
        ")\n",
        "class_weights = dict(zip(np.unique(train_l), cw))"
      ],
      "metadata": {
        "id": "DV1K3ZxyMYpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DNN"
      ],
      "metadata": {
        "id": "_N5y5fUXVWbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_nn_model(input_shape):\n",
        "    # input layer\n",
        "    inp = Input(shape = (input_shape,))\n",
        "\n",
        "    # first hidden layer\n",
        "    x = Dense(256, kernel_initializer = 'he_normal')(inp)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha = 0.05)(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    # second hidden layer\n",
        "    x = Dense(512, kernel_initializer = 'he_normal')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha = 0.05)(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    # third hidden layer\n",
        "    x = Dense(1024, kernel_initializer = 'he_normal')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha = 0.05)(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    # fourth hidden layer\n",
        "    x = Dense(512, kernel_initializer = 'he_normal')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha = 0.05)(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    # fifth hidden layer\n",
        "    x = Dense(256, kernel_initializer = 'he_normal')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha = 0.05)(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    # sixth hidden layer\n",
        "    x = Dense(128, kernel_initializer = 'he_normal')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha = 0.05)(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    # seventh hidden layer\n",
        "    x = Dense(64, kernel_initializer = 'he_normal')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha = 0.05)(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    # output layer\n",
        "    out = Dense(1, activation = 'sigmoid')(x)\n",
        "    model = Model(inputs = inp, outputs = out)\n",
        "    return model"
      ],
      "metadata": {
        "id": "H2vhIbgyJmJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_model = create_nn_model(length)\n",
        "nn_model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['AUC'])\n",
        "\n",
        "nn_model.fit(X_train, y_train, epochs=40, batch_size=32,\n",
        "           class_weight=class_weights)\n"
      ],
      "metadata": {
        "id": "OlBXgEDKMaHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_pred_probs = nn_model.predict(X_test)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_probs)\n",
        "prc_auc = average_precision_score(y_test, y_pred_probs)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"AUC-ROC:\", roc_auc)\n",
        "print(\"PR-PRC:\", prc_auc)"
      ],
      "metadata": {
        "id": "7_8_XSwYJmHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#XGB"
      ],
      "metadata": {
        "id": "QGqu95XFOZuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if dataset_name == 'clintox':\n",
        "    xgb_model = xgb.XGBClassifier(\n",
        "        objective='binary:logistic',\n",
        "        eval_metric='logloss',\n",
        "        max_depth=8,\n",
        "        learning_rate=0.1,\n",
        "        n_estimators=200,\n",
        "        subsample=0.6,\n",
        "        colsample_bytree=0.7,\n",
        "        random_state=42,\n",
        "        reg_lambda=0,\n",
        "        reg_alpha=0,\n",
        "        min_child_weight=1,\n",
        "        tree_method='gpu_hist',\n",
        "        gpu_id=0\n",
        ")\n",
        "\n",
        "elif dataset_name == 'cardio':\n",
        "    xgb_model = xgb.XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='logloss',\n",
        "    max_depth=5,\n",
        "    learning_rate=0.3,\n",
        "    n_estimators=1000,\n",
        "    subsample=1.0,\n",
        "    colsample_bytree=0.6,\n",
        "    random_state=42,\n",
        "    reg_lambda=1,\n",
        "    reg_alpha=0,\n",
        "    min_child_weight=1,\n",
        "    tree_method='gpu_hist',\n",
        "    gpu_id=0\n",
        ")\n",
        "\n",
        "else: # tox21\n",
        "    xgb_model = xgb.XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='logloss',\n",
        "    max_depth=9,\n",
        "    learning_rate=0.1,\n",
        "    n_estimators=500,\n",
        "    subsample=0.6,\n",
        "    colsample_bytree=0.6,\n",
        "    random_state=42,\n",
        "    reg_lambda=1,\n",
        "    reg_alpha=0.1,\n",
        "    min_child_weight=1,\n",
        "    tree_method='gpu_hist',\n",
        "    gpu_id=0,\n",
        ")\n",
        "xgb_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "OARo0DKFJmFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = xgb_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred)\n",
        "prc_auc = average_precision_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"AUC-ROC:\", roc_auc)\n",
        "print(\"PR-PRC:\", prc_auc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEVL4i11JmDm",
        "outputId": "f90b1121-d191-46d0-bef0-2a177a83963d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8612903225806452\n",
            "Precision: 0.6578947368421053\n",
            "Recall: 0.746268656716418\n",
            "AUC-ROC: 0.8196363859713778\n",
            "PR-PRC: 0.5458049312013785\n"
          ]
        }
      ]
    }
  ]
}