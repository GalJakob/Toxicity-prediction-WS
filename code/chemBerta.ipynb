{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GalJakob/Toxicity-prediction-WS/blob/main/code/chemBerta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8aoAILxZLni"
      },
      "source": [
        "***import datasets***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrN__9NFaY00",
        "outputId": "7fda1eea-25c2-4488-bdda-140d6a0362c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import io\n",
        "import pandas as pd\n",
        "from google.colab import files,drive\n",
        "\n",
        "WITH_TRAINING = False #change to True if training is needed\n",
        "WITH_HYPER_PARAMS = True # change to True if testing hyper parameters is needed\n",
        "MODEL = \"chemBerta\" # constant\n",
        "AUGMENTED_CASE = \"none augmented\"  # can be \"none augmented\" / \"only train augmented\"/ \"both augmented\"\n",
        "dataset_name = \"clintox\" # can be cardio / tox21 / clintox\n",
        "ds_test = None\n",
        "ds_train = None\n",
        "\n",
        "if AUGMENTED_CASE == \"none augmented\":\n",
        "  ds_test = dataset_name + \"_test\"\n",
        "  ds_train = dataset_name + \"_train\"\n",
        "elif AUGMENTED_CASE == \"only train augmented\":\n",
        "  ds_test = dataset_name + \"_test\"\n",
        "  ds_train = dataset_name + \"_train_aug\"\n",
        "else:\n",
        "  ds_test = dataset_name + \"_test\"\n",
        "  ds_train = dataset_name + \"_train_aug\"\n",
        "\n",
        "path_train = f\"https://raw.githubusercontent.com/GalJakob/Toxicity-prediction-WS/main/datasets/train%20datasets/{ds_train}.csv\"\n",
        "path_test = f\"https://raw.githubusercontent.com/GalJakob/Toxicity-prediction-WS/main/datasets/test%20datasets/{ds_test}.csv\"\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "try: #getting data from github\n",
        "  test_data = pd.read_csv(path_test)\n",
        "  train_data = pd.read_csv(path_train)\n",
        "\n",
        "except: #uploading data instead from github\n",
        "  data = files.upload()\n",
        "  train_data = io.BytesIO(data[ds_train])\n",
        "  test_data = io.BytesIO(data[ds_test])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eHUvGPRZYeh"
      },
      "source": [
        "***installation required***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyZs4MvmifaC"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/seyonechithrananda/bert-loves-chemistry.git\n",
        "!pip install transformers\n",
        "!pip install simpletransformers\n",
        "!pip install --pre deepchem\n",
        "!pip install datasets scipy sklearn torch tqdm wandb\n",
        "%cd /content/bert-loves-chemistry\n",
        "!wget https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/vocab.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSbhQyDiR1WV"
      },
      "source": [
        "***split data  (80/10/10)***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcUi-kvVPRKm"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "path_original_dataset = f\"https://raw.githubusercontent.com/GalJakob/Toxicity-prediction-WS/main/datasets/original%20datasets/{dataset_name}.csv\"\n",
        "data = pd.read_csv(path_original_dataset)\n",
        "\n",
        "  if AUGMENTED_CASE == \"none augmented\":\n",
        "    X_train, X_test, y_train, y_test = train_test_split(data[\"smiles\"], data[\"label\"], test_size=0.2, random_state=42,shuffle = True)\n",
        "    X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42,shuffle = True)\n",
        "    train_data = pd.DataFrame({\"smiles\": X_train, \"label\": y_train})\n",
        "    test_data = pd.DataFrame({\"smiles\": X_test, \"label\": y_test})\n",
        "    val_data = pd.DataFrame({\"smiles\": X_val, \"label\": y_val})\n",
        "\n",
        "  else:#split only test_data\n",
        "    X_test, X_val, y_test, y_val = train_test_split(test_data[\"smiles\"], test_data[\"label\"], test_size=0.5, random_state=42,shuffle = True)\n",
        "    test_data = pd.DataFrame({\"smiles\": X_test, \"label\": y_test})\n",
        "    val_data = pd.DataFrame({\"smiles\": X_val, \"label\": y_val})\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-qrXThgYGCL"
      },
      "source": [
        "***model builder***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I634STgteKLR"
      },
      "outputs": [],
      "source": [
        "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
        "import torch\n",
        "import sklearn\n",
        "model_args = ClassificationArgs()\n",
        "model_args.train_batch_size = 16\n",
        "model_args.evaluate_during_training = WITH_HYPER_PARAMS\n",
        "model_args.evaluate_during_training_silent = False\n",
        "model_args.evaluate_during_training_steps = -1\n",
        "model_args.save_eval_checkpoints = False\n",
        "model_args.save_model_every_epoch = False\n",
        "model_args.learning_rate = 0.0000243\n",
        "model_args.manual_seed = 4\n",
        "model_args.no_cache = True\n",
        "model_args.num_train_epochs = 35\n",
        "model_args.overwrite_output_dir = True\n",
        "model_args.reprocess_input_data = True\n",
        "model_args.output_dir = \"default_output\"\n",
        "model_args.best_model_dir = \"default_output/best_model\"\n",
        "model_args.auto_weights = True\n",
        "\n",
        "\n",
        "model = ClassificationModel('roberta',\n",
        "                            'seyonec/PubChem10M_SMILES_BPE_396_250',\n",
        "                            use_cuda = torch.cuda.is_available(),\n",
        "                            args=model_args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPI4XnU9YjKw"
      },
      "source": [
        "***hyperparameter tuning ***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlDImIwgjcVW"
      },
      "outputs": [],
      "source": [
        "### hyperparameter imports ###\n",
        "from datasets import load_dataset, load_metric\n",
        "from transformers import (AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments)\n",
        "import wandb\n",
        "\n",
        "### chemberta imports ###\n",
        "from rdkit import Chem\n",
        "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
        "\n",
        "sweep_config = {\n",
        "    \"name\": \"chemBerta\",\n",
        "    \"method\": \"bayes\",\n",
        "    \"metric\": {\"name\": \"auprc\", \"goal\": \"maximize\"},\n",
        "    \"parameters\": {\n",
        "        \"num_train_epochs\": {\"min\": 10, \"max\": 30},\n",
        "        \"learning_rate\": {\"min\": 0.0000001, \"max\": 0.001},\n",
        "         \"batch_size\": {\"values\": [32,16]},\n",
        "    },\n",
        "    \"early_terminate\": {\"type\": \"hyperband\", \"min_iter\": 6,},\n",
        "}\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"chemBerta\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fxOj6DGtlH7F"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "from sklearn.metrics import accuracy_score\n",
        "def train_for_hyper_params():\n",
        "  wandb.init()\n",
        "  model = ClassificationModel('roberta',\n",
        "                            'seyonec/PubChem10M_SMILES_BPE_396_250',\n",
        "                            use_cuda = torch.cuda.is_available(),\n",
        "                            args=model_args,\n",
        "                            sweep_config=wandb.config)\n",
        "\n",
        "  model.train_model(train_data, eval_df=val_data,\n",
        "                    accuracy=lambda truth, predictions: accuracy_score(truth, [round(p) for p in predictions]) )\n",
        "  wandb.join()\n",
        "\n",
        "wandb.agent(sweep_id,train_for_hyper_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-hEH0TAaUEs"
      },
      "source": [
        "***training***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rvO-ebNeKJB"
      },
      "outputs": [],
      "source": [
        "#training if wanted\n",
        "if WITH_TRAINING:\n",
        "  model.train_model(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoYXV93XaX8a"
      },
      "source": [
        "***augmentation builder code, essential for majority vote code***\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bRryXiUsMRa"
      },
      "outputs": [],
      "source": [
        "from rdkit import Chem\n",
        "import numpy as np\n",
        "import threading\n",
        "\n",
        "class Iterator(object):\n",
        "    \"\"\"Abstract base class for data iterators.\n",
        "\n",
        "    # Arguments\n",
        "        n: Integer, total number of samples in the dataset to loop over.\n",
        "        batch_size: Integer, size of a batch.\n",
        "        shuffle: Boolean, whether to shuffle the data between epochs.\n",
        "        seed: Random seeding for data shuffling.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n, batch_size, shuffle, seed):\n",
        "        self.n = n\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.batch_index = 0\n",
        "        self.total_batches_seen = 0\n",
        "        self.lock = threading.Lock()\n",
        "        self.index_generator = self._flow_index(n, batch_size, shuffle, seed)\n",
        "        if n < batch_size:\n",
        "            raise ValueError('Input data length is shorter than batch_size\\nAdjust batch_size')\n",
        "\n",
        "    def reset(self):\n",
        "        self.batch_index = 0\n",
        "\n",
        "    def _flow_index(self, n, batch_size=32, shuffle=False, seed=None):\n",
        "        # Ensure self.batch_index is 0.\n",
        "        self.reset()\n",
        "        while 1:\n",
        "            if seed is not None:\n",
        "                np.random.seed(seed + self.total_batches_seen)\n",
        "            if self.batch_index == 0:\n",
        "                index_array = np.arange(n)\n",
        "                if shuffle:\n",
        "                    index_array = np.random.permutation(n)\n",
        "\n",
        "            current_index = (self.batch_index * batch_size) % n\n",
        "            if n > current_index + batch_size:\n",
        "                current_batch_size = batch_size\n",
        "                self.batch_index += 1\n",
        "            else:\n",
        "                current_batch_size = n - current_index\n",
        "                self.batch_index = 0\n",
        "            self.total_batches_seen += 1\n",
        "            yield (index_array[current_index: current_index + current_batch_size],\n",
        "                   current_index, current_batch_size)\n",
        "\n",
        "    def __iter__(self):\n",
        "        # Needed if we want to do something like:\n",
        "        # for x, y in data_gen.flow(...):\n",
        "        return self\n",
        "\n",
        "    def __next__(self, *args, **kwargs):\n",
        "        return self.next(*args, **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class SmilesIterator(Iterator):\n",
        "    \"\"\"Iterator yielding data from a SMILES array.\n",
        "\n",
        "    # Arguments\n",
        "        x: Numpy array of SMILES input data.\n",
        "        y: Numpy array of targets data.\n",
        "        smiles_data_generator: Instance of `SmilesEnumerator`\n",
        "            to use for random SMILES generation.\n",
        "        batch_size: Integer, size of a batch.\n",
        "        shuffle: Boolean, whether to shuffle the data between epochs.\n",
        "        seed: Random seed for data shuffling.\n",
        "        dtype: dtype to use for returned batch. Set to keras.backend.floatx if using Keras\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, x, y, smiles_data_generator,\n",
        "                 batch_size=32, shuffle=False, seed=None,\n",
        "                 dtype=np.float32\n",
        "                 ):\n",
        "        if y is not None and len(x) != len(y):\n",
        "            raise ValueError('X (images tensor) and y (labels) '\n",
        "                             'should have the same length. '\n",
        "                             'Found: X.shape = %s, y.shape = %s' %\n",
        "                             (np.asarray(x).shape, np.asarray(y).shape))\n",
        "\n",
        "        self.x = np.asarray(x)\n",
        "\n",
        "        if y is not None:\n",
        "            self.y = np.asarray(y)\n",
        "        else:\n",
        "            self.y = None\n",
        "        self.smiles_data_generator = smiles_data_generator\n",
        "        self.dtype = dtype\n",
        "        super(SmilesIterator, self).__init__(x.shape[0], batch_size, shuffle, seed)\n",
        "\n",
        "    def next(self):\n",
        "        \"\"\"For python 2.x.\n",
        "\n",
        "        # Returns\n",
        "            The next batch.\n",
        "        \"\"\"\n",
        "        # Keeps under lock only the mechanism which advances\n",
        "        # the indexing of each batch.\n",
        "        with self.lock:\n",
        "            index_array, current_index, current_batch_size = next(self.index_generator)\n",
        "        # The transformation of images is not under thread lock\n",
        "        # so it can be done in parallel\n",
        "        batch_x = np.zeros(tuple([current_batch_size] + [ self.smiles_data_generator.pad, self.smiles_data_generator._charlen]), dtype=self.dtype)\n",
        "        for i, j in enumerate(index_array):\n",
        "            smiles = self.x[j:j+1]\n",
        "            x = self.smiles_data_generator.transform(smiles)\n",
        "            batch_x[i] = x\n",
        "\n",
        "        if self.y is None:\n",
        "            return batch_x\n",
        "        batch_y = self.y[index_array]\n",
        "        return batch_x, batch_y\n",
        "\n",
        "\n",
        "class SmilesEnumerator(object):\n",
        "    \"\"\"SMILES Enumerator, vectorizer and devectorizer\n",
        "\n",
        "    #Arguments\n",
        "        charset: string containing the characters for the vectorization\n",
        "          can also be generated via the .fit() method\n",
        "        pad: Length of the vectorization\n",
        "        leftpad: Add spaces to the left of the SMILES\n",
        "        isomericSmiles: Generate SMILES containing information about stereogenic centers\n",
        "        enum: Enumerate the SMILES during transform\n",
        "        canonical: use canonical SMILES during transform (overrides enum)\n",
        "    \"\"\"\n",
        "    def __init__(self, charset = '@C)(=cOn1S2/H[N]\\\\', pad=120, leftpad=True, isomericSmiles=True, enum=True, canonical=False):\n",
        "        self._charset = None\n",
        "        self.charset = charset\n",
        "        self.pad = pad\n",
        "        self.leftpad = leftpad\n",
        "        self.isomericSmiles = isomericSmiles\n",
        "        self.enumerate = enum\n",
        "        self.canonical = canonical\n",
        "\n",
        "    @property\n",
        "    def charset(self):\n",
        "        return self._charset\n",
        "\n",
        "    @charset.setter\n",
        "    def charset(self, charset):\n",
        "        self._charset = charset\n",
        "        self._charlen = len(charset)\n",
        "        self._char_to_int = dict((c,i) for i,c in enumerate(charset))\n",
        "        self._int_to_char = dict((i,c) for i,c in enumerate(charset))\n",
        "\n",
        "    def fit(self, smiles, extra_chars=[], extra_pad = 5):\n",
        "        \"\"\"Performs extraction of the charset and length of a SMILES datasets and sets self.pad and self.charset\n",
        "\n",
        "        #Arguments\n",
        "            smiles: Numpy array or Pandas series containing smiles as strings\n",
        "            extra_chars: List of extra chars to add to the charset (e.g. \"\\\\\\\\\" when \"/\" is present)\n",
        "            extra_pad: Extra padding to add before or after the SMILES vectorization\n",
        "        \"\"\"\n",
        "        charset = set(\"\".join(list(smiles)))\n",
        "        self.charset = \"\".join(charset.union(set(extra_chars)))\n",
        "        self.pad = max([len(smile) for smile in smiles]) + extra_pad\n",
        "\n",
        "    def randomize_smiles(self, smiles):\n",
        "        \"\"\"Perform a randomization of a SMILES string\n",
        "        must be RDKit sanitizable\"\"\"\n",
        "        m = Chem.MolFromSmiles(smiles)\n",
        "        ans = list(range(m.GetNumAtoms()))\n",
        "        np.random.shuffle(ans)\n",
        "        nm = Chem.RenumberAtoms(m,ans)\n",
        "        return Chem.MolToSmiles(nm, canonical=self.canonical, isomericSmiles=self.isomericSmiles)\n",
        "\n",
        "    def transform(self, smiles):\n",
        "        \"\"\"Perform an enumeration (randomization) and vectorization of a Numpy array of smiles strings\n",
        "        #Arguments\n",
        "            smiles: Numpy array or Pandas series containing smiles as strings\n",
        "        \"\"\"\n",
        "        one_hot =  np.zeros((smiles.shape[0], self.pad, self._charlen),dtype=np.int8)\n",
        "\n",
        "        if self.leftpad:\n",
        "            for i,ss in enumerate(smiles):\n",
        "                if self.enumerate: ss = self.randomize_smiles(ss)\n",
        "                l = len(ss)\n",
        "                diff = self.pad - l\n",
        "                for j,c in enumerate(ss):\n",
        "                    one_hot[i,j+diff,self._char_to_int[c]] = 1\n",
        "            return one_hot\n",
        "        else:\n",
        "            for i,ss in enumerate(smiles):\n",
        "                if self.enumerate: ss = self.randomize_smiles(ss)\n",
        "                for j,c in enumerate(ss):\n",
        "                    one_hot[i,j,self._char_to_int[c]] = 1\n",
        "            return one_hot\n",
        "\n",
        "\n",
        "    def reverse_transform(self, vect):\n",
        "        \"\"\" Performs a conversion of a vectorized SMILES to a smiles strings\n",
        "        charset must be the same as used for vectorization.\n",
        "        #Arguments\n",
        "            vect: Numpy array of vectorized SMILES.\n",
        "        \"\"\"\n",
        "        smiles = []\n",
        "        for v in vect:\n",
        "            #mask v\n",
        "            v=v[v.sum(axis=1)==1]\n",
        "            #Find one hot encoded index with argmax, translate to char and join to string\n",
        "            smile = \"\".join(self._int_to_char[i] for i in v.argmax(axis=1))\n",
        "            smiles.append(smile)\n",
        "        return np.array(smiles)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    smiles = np.array([ \"CCC(=O)O[C@@]1(CC[NH+](C[C@H]1CC=C)C)c2ccccc2\",\n",
        "                        \"CCC[S@@](=O)c1ccc2c(c1)[nH]/c(=N/C(=O)OC)/[nH]2\"]*10\n",
        "                        )\n",
        "    #Test canonical SMILES vectorization\n",
        "    sm_en = SmilesEnumerator(canonical=True, enum=False)\n",
        "    sm_en.fit(smiles, extra_chars=[\"\\\\\"])\n",
        "    v = sm_en.transform(smiles)\n",
        "    transformed = sm_en.reverse_transform(v)\n",
        "    if len(set(transformed)) > 2: print(\"Too many different canonical SMILES generated\")\n",
        "\n",
        "    #Test enumeration\n",
        "    sm_en.canonical = False\n",
        "    sm_en.enumerate = True\n",
        "    v2 = sm_en.transform(smiles)\n",
        "    transformed = sm_en.reverse_transform(v2)\n",
        "    if len(set(transformed)) < 3: print(\"Too few enumerated SMILES generated\")\n",
        "\n",
        "    #Reconstruction\n",
        "    reconstructed = sm_en.reverse_transform(v[0:5])\n",
        "    for i, smile in enumerate(reconstructed):\n",
        "        if smile != smiles[i]:\n",
        "            print(\"Error in reconstruction %s %s\"%(smile, smiles[i]))\n",
        "            break\n",
        "\n",
        "    #test Pandas\n",
        "    import pandas as pd\n",
        "    df = pd.DataFrame(smiles)\n",
        "    v = sm_en.transform(df[0])\n",
        "    if v.shape != (20, 52, 18): print(\"Possible error in pandas use\")\n",
        "\n",
        "    #BUG, when batchsize > x.shape[0], then it only returns x.shape[0]!\n",
        "    #Test batch generation\n",
        "    sm_it = SmilesIterator(smiles, np.array([1,2]*10), sm_en, batch_size=10, shuffle=True)\n",
        "    X, y = sm_it.next()\n",
        "    if sum(y==1) - sum(y==2) > 1:\n",
        "        print(\"Unbalanced generation of batches\")\n",
        "    if len(X) != 10: print(\"Error in batchsize generation\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKg0K1-Fagvz"
      },
      "source": [
        "***eval by majority(both augmented) functions***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62g8ZOxLsDWZ"
      },
      "outputs": [],
      "source": [
        "from rdkit import Chem\n",
        "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "\n",
        "def get_idx_of_matched_smile(list_of_dict_smiles,curr_smile):\n",
        "  ''' returns the idx of the smile of different smile but same molecule,\n",
        "    returns None if not found'''\n",
        "  if len(list_of_dict_smiles) == 0:\n",
        "    return None\n",
        "\n",
        "  for idx in range(len(list_of_dict_smiles)):\n",
        "    curr_list_smile = list(list_of_dict_smiles[idx].keys())[0]\n",
        "    curr_smile_obj = Chem.MolFromSmiles(curr_smile)\n",
        "    curr_list_smile_obj = Chem.MolFromSmiles(curr_list_smile)\n",
        "\n",
        "    if curr_smile_obj.HasSubstructMatch(curr_list_smile_obj) and curr_list_smile_obj.HasSubstructMatch(curr_smile_obj): #smiles are same molecule\n",
        "      return idx\n",
        "\n",
        "  return None\n",
        "\n",
        "\n",
        "def build_list_dict_of_smiles(predictions,smiles,true_labels):\n",
        "  ''' returns the list: [{\"randmon_smile\":{\"count_all\":#,\"count_correct\":#,\"true_label\":0/1  }},{}]   '''\n",
        "  list_of_dict_smiles=[]\n",
        "\n",
        "  for idx in range(len(predictions)):\n",
        "    curr_smile = smiles[idx]\n",
        "    matched_idx = get_idx_of_matched_smile(list_of_dict_smiles,curr_smile)\n",
        "\n",
        "    if matched_idx == None: #molecule not in list->add it\n",
        "      one_for_correct = 0\n",
        "      if predictions[idx] == true_labels[idx]:\n",
        "        one_for_correct = 1\n",
        "      list_of_dict_smiles.append({f\"{curr_smile}\":{\"count_all\":1,\"count_correct\":one_for_correct,\"true_label\": true_labels[idx] }})\n",
        "\n",
        "    else: #molecule in list\n",
        "      curr_obj_mol = list_of_dict_smiles[matched_idx]\n",
        "      smile0 = list(curr_obj_mol.keys())[0]\n",
        "      curr_obj_mol[smile0][\"count_all\"] = curr_obj_mol[smile0][\"count_all\"] +1\n",
        "      if predictions[idx] == true_labels[idx]:\n",
        "        curr_obj_mol[smile0][\"count_correct\"] = curr_obj_mol[smile0][\"count_correct\"] +1\n",
        "  return list_of_dict_smiles\n",
        "\n",
        "def ensure_dup_is_not_orginial(original_SMILE,dup_SMILE,sme):\n",
        "  counter = 0\n",
        "  while(True):\n",
        "    if original_SMILE != dup_SMILE:\n",
        "      return dup_SMILE\n",
        "    dup_SMILE = sme.randomize_smiles(original_SMILE)\n",
        "    counter+=1\n",
        "    if counter > 1000:\n",
        "    # raise Exception(\"Something wrong with SMILE duplicator\")\n",
        "      print(\"over 1000 attempts to duplicate\")\n",
        "      return dup_SMILE\n",
        "\n",
        "def evaluate_by_majority(predictions_of_dup_SMILE,true_label,dup_count,threshold):\n",
        "  '''gets predictions for specific SMILE duplicates and returns the prediction for original SMILE '''\n",
        "  cnt_correct = 0\n",
        "  for prediction in predictions_of_dup_SMILE:\n",
        "    if prediction == true_label:\n",
        "      cnt_correct+=1\n",
        "\n",
        "  if ((cnt_correct/dup_count) >= threshold ) and true_label == 1:\n",
        "      return 1\n",
        "  if ((cnt_correct/dup_count) >= threshold ) and true_label == 0:\n",
        "      return 0\n",
        "  if ((cnt_correct/dup_count) < threshold ) and true_label == 1:\n",
        "      return 0\n",
        "  if ((cnt_correct/dup_count) < threshold ) and true_label == 0:\n",
        "      return 1\n",
        "\n",
        "def predictions_by_majority(model,test_data):\n",
        "  '''duplicates some of the SMILES and then lets model preform prediction on each duplicated SMILE and combines\n",
        "   them to a prediction by some threshold and finally to a list'''\n",
        "  threshold = 0.6\n",
        "  predictions = []\n",
        "  predictions_of_dup_SMILE=[]\n",
        "  dup_SMILEs_list=[]\n",
        "  sme = SmilesEnumerator()\n",
        "  true_labels = test_data[\"label\"].values.tolist()\n",
        "  dict_of_dup_weights= {'label_1':{'dups':[10],'probs':[1]},\n",
        "                        'label_0':{'dups':[10],'probs':[1]},}\n",
        "\n",
        "  for i in range(len(true_labels)):\n",
        "    original_SMILE = test_data['smiles'][i]\n",
        "    num_of_duplicates = None\n",
        "\n",
        "    if test_data['label'][i]  == 1:\n",
        "        num_of_duplicates = np.random.choice(dict_of_dup_weights['label_1']['dups'], p=dict_of_dup_weights['label_1']['probs'])\n",
        "        for idx_of_dup in range(num_of_duplicates):\n",
        "            dup_SMILE = sme.randomize_smiles(original_SMILE)\n",
        "            if dup_SMILE == original_SMILE:\n",
        "              dup_SMILE = ensure_dup_is_not_orginial(original_SMILE,dup_SMILE,sme)\n",
        "            dup_SMILEs_list.append(dup_SMILE)\n",
        "\n",
        "    else:\n",
        "        num_of_duplicates = np.random.choice(dict_of_dup_weights['label_0']['dups'], p=dict_of_dup_weights['label_0']['probs'])\n",
        "        for idx_of_dup in range(num_of_duplicates):\n",
        "            dup_SMILE = sme.randomize_smiles(original_SMILE)\n",
        "            if dup_SMILE == original_SMILE:\n",
        "              dup_SMILE = ensure_dup_is_not_orginial(original_SMILE,dup_SMILE,sme)\n",
        "            dup_SMILEs_list.append(original_SMILE)\n",
        "\n",
        "    predictions_of_dup_SMILE = list(model.predict(dup_SMILEs_list)[0])\n",
        "    prediction_for_original_SMILE = evaluate_by_majority(predictions_of_dup_SMILE,test_data['label'][i],num_of_duplicates,threshold)\n",
        "    predictions.append(prediction_for_original_SMILE)\n",
        "    dup_SMILEs_list=[]\n",
        "\n",
        "  accuracy = metrics.accuracy_score(true_labels,predictions)\n",
        "  roc_auc = metrics.roc_auc_score(true_labels,predictions)\n",
        "  precision_list, recall_list, thresholds = metrics.precision_recall_curve(true_labels,predictions)\n",
        "  pr_auc = metrics.auc(recall_list, precision_list)\n",
        "  precision = metrics.precision_score(true_labels,predictions)\n",
        "  recall = metrics.recall_score(true_labels,predictions)\n",
        "  tn, fp, fn, tp = metrics.confusion_matrix(true_labels, predictions).ravel()\n",
        "\n",
        "  result = {'acc':accuracy,'auroc':roc_auc,'auprc':pr_auc,'precision':precision,'recall':recall,\n",
        "            'tp':tp,'fp':fp,'fn':fn,'tn':tn,'threshold':threshold,'dict_of_dup_weights':dict_of_dup_weights}\n",
        "  return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEMW7moXa3Sg"
      },
      "source": [
        "***evaluate and write results to files***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKAV9_GBeKGp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from rdkit import Chem\n",
        "import pickle\n",
        "import sklearn\n",
        "from sklearn import metrics\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "result = None\n",
        "model_outputs = None\n",
        "wrong_predictions = None\n",
        "loaded_model = None\n",
        "predictions=[]\n",
        "true_labels = list(test_data[\"label\"])\n",
        "\n",
        "if not (AUGMENTED_CASE == \"both augmented\"):\n",
        "  result, model_outputs, wrong_predictions = model.eval_model(test_data, acc=sklearn.metrics.accuracy_score)\n",
        "  predictions = list(model.predict(list(test_data[\"smiles\"]))[0])\n",
        "\n",
        "  result[\"precision\"] = metrics.precision_score(true_labels,predictions)\n",
        "\n",
        "  result[\"recall\"]= metrics.recall_score(true_labels,predictions)\n",
        "\n",
        "else: # evaluate by majority vote\n",
        "\n",
        "  if not WITH_TRAINING:\n",
        "    model_pkl_file = f\"/content/drive/MyDrive/Toxicity prediction WS/models/{AUGMENTED_CASE}/{MODEL}/{dataset_name}_model.pkl\"\n",
        "\n",
        "    if device.type == \"cuda\":\n",
        "      loaded_model = pickle.load(open(model_pkl_file, 'rb')) #IF CUDE AVAILABLE\n",
        "    else:\n",
        "       loaded_model = torch.load(model_pkl_file, map_location=device)\n",
        "    result = predictions_by_majority(loaded_model,test_data)\n",
        "\n",
        "  else:\n",
        "    result = predictions_by_majority(model,test_data)\n",
        "\n",
        "\n",
        "result['f1_score'] = 2 * (result['precision'] *result['recall']) / (result['recall'] + result['precision'])\n",
        "\n",
        "print(result)\n",
        "\n",
        "with open(f\"/content/drive/MyDrive/Toxicity prediction WS/results/{AUGMENTED_CASE}/{MODEL}/{dataset_name}_results.txt\", 'w') as resFile:\n",
        "    resFile.write(f\"results for {ds_test} :\\n\")\n",
        "    resFile.write(f\"accuracy: {result['acc']} \\n\")\n",
        "    resFile.write(f\"precision: {result['precision']} \\n\")\n",
        "    resFile.write(f\"recall: {result['recall']} \\n\")\n",
        "    resFile.write(f\"Area under the ROC curve: {result['auroc']}\\n\")\n",
        "    resFile.write(f\"Area under the PR curve: {result['auprc']}\\n\")\n",
        "    resFile.write(f\"confusion matrix:  \\n\")\n",
        "    resFile.write(f\"true positive :{result['tp']},false positive:{result['fp']} \\n\")\n",
        "    resFile.write(f\"false negative :{result['fn']},true negative:{result['tn']} \\n\")\n",
        "    resFile.write(f\"F1 Score: {result['f1_score']}\\n\")\n",
        "    if AUGMENTED_CASE == \"both augmented\":\n",
        "      resFile.write(f\"threshold: {result['threshold']} \\n\")\n",
        "      resFile.write(f\"threshold: {result['dict_of_dup_weights']} \\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDe6hwwabBiS"
      },
      "source": [
        "***save the model as a pickle file***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pigWFSSy_wK"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "model_pkl_file = f\"../drive/MyDrive/Toxicity prediction WS/models/{AUGMENTED_CASE}/{MODEL}/{dataset_name}_model.pkl\"\n",
        "\n",
        "with open(model_pkl_file, 'wb') as file:\n",
        "   pickle.dump(model, file)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs4DCC_ZbK7M"
      },
      "source": [
        "***load the model as a pickle file and predict with example***\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53Jpr_TBB2bk"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import sklearn\n",
        "model_pkl_file = f\"/content/drive/MyDrive/Toxicity prediction WS/models/{AUGMENTED_CASE}/{MODEL}/{dataset_name}_model.pkl\"\n",
        "\n",
        "loaded_model = pickle.load(open(model_pkl_file, 'rb'))\n",
        "result, model_outputs, wrong_predictions = loaded_model.eval_model(test_data, acc=sklearn.metrics.accuracy_score)\n",
        "print(result)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}