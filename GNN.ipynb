{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GalJakob/Toxicity-prediction-WS/blob/main/Non_pretrained_gnn_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install Pytorch Geometric => Build Graph Neural Network\n",
        "import torch\n",
        "pytorch_version = f\"torch-{torch.__version__}.html\"\n",
        "!pip install --no-index torch-scatter -f https://pytorch-geometric.com/whl/$pytorch_version\n",
        "!pip install --no-index torch-sparse -f https://pytorch-geometric.com/whl/$pytorch_version\n",
        "!pip install --no-index torch-cluster -f https://pytorch-geometric.com/whl/$pytorch_version\n",
        "!pip install --no-index torch-spline-conv -f https://pytorch-geometric.com/whl/$pytorch_version\n",
        "!pip install torch-geometric"
      ],
      "metadata": {
        "id": "ULZr8TPXWEYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install RDKit => Handle Molecule Data\n",
        "from logging import getLogger, StreamHandler, INFO\n",
        "\n",
        "logger = getLogger(__name__)\n",
        "logger.addHandler(StreamHandler())\n",
        "logger.setLevel(INFO)\n",
        "\n",
        "# !curl -Lo conda_installer.py https://raw.githubusercontent.com/deepchem/deepchem/master/scripts/colab_install.py\n",
        "# import conda_installer\n",
        "# conda_installer.install() # takes ~ 4 minutes #TODO: pre-install it\n",
        "# !/root/miniconda/bin/conda info -e\n",
        "\n",
        "# !pip install --pre deepchem\n",
        "# import deepchem\n",
        "# deepchem.__version__\n",
        "!pip install rdkit\n",
        "import rdkit\n",
        "logger.info(\"rdkit-{} installation finished!\".format(rdkit.__version__))"
      ],
      "metadata": {
        "id": "UOQ2hpRiXfmV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3843be5-5220-4b76-a1cf-64d219fc7b75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.10/dist-packages (2023.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (9.4.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "rdkit-2023.03.2 installation finished!\n",
            "rdkit-2023.03.2 installation finished!\n",
            "INFO:__main__:rdkit-2023.03.2 installation finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrN__9NFaY00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f0d8059-4533-4f6b-dca0-ca13309fc48d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "only train augmented\n"
          ]
        }
      ],
      "source": [
        "#load data\n",
        "import io\n",
        "import pandas as pd\n",
        "from google.colab import files,drive\n",
        "\n",
        "AUGMENTED_CASE = \"none augmented\"  # can be \"none augmented\" / \"only train augmented\"/ \"both augmented\"\n",
        "GNN = \"GNN\" # constant\n",
        "dataset_name = \"cardio\" # change to cardio / tox21 / clintox\n",
        "ds_test = dataset_name + \"_test\" # change to _test / _test_aug\n",
        "ds_train = dataset_name + \"_train_aug\" # change to _train / _train_aug\n",
        "path_test = f\"/content/drive/MyDrive/workshop/datasets/test datasets/{ds_test}.csv\" #data is at google drive\n",
        "path_train = f\"/content/drive/MyDrive/workshop/datasets/train datasets/{ds_train}.csv\" #data is at google drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "try: #getting data from drive\n",
        "  test_data = pd.read_csv(path_test)\n",
        "  train_data = pd.read_csv(path_train)\n",
        "\n",
        "except: #uploading data instead from drive\n",
        "  data = files.upload()\n",
        "  train_data = io.BytesIO(data[ds_train])\n",
        "  test_data = io.BytesIO(data[ds_test])\n",
        "\n",
        "if (\"_train_aug\" in ds_train) and (\"_test_aug\" in ds_test):\n",
        "  AUGMENTED_CASE = \"both augmented\"\n",
        "elif (\"_train_aug\" in ds_train) and (not (\"_test_aug\" in ds_test)):\n",
        "  AUGMENTED_CASE = \"only train augmented\"\n",
        "elif (\"_train\" in ds_train) and (\"_test_aug\" in ds_test):\n",
        "  print(\"ERROR, only test is augmented\")\n",
        "\n",
        "print(AUGMENTED_CASE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#convert smiles to graphs\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw\n",
        "from rdkit.Chem.Draw import IPythonConsole\n",
        "import pandas as pd\n",
        "\n",
        "mol_objs_train = [Chem.MolFromSmiles(SMILE) for SMILE in train_data[\"smiles\"]]\n",
        "train_data_by_mols = pd.DataFrame({\"mol_objs\":mol_objs_train, \"label\": train_data[\"label\"]})\n",
        "\n",
        "mol_objs = [Chem.MolFromSmiles(SMILE) for SMILE in test_data[\"smiles\"]]\n",
        "test_data_by_mols = pd.DataFrame({\"mol_objs\":mol_objs, \"label\": test_data[\"label\"]})\n",
        "\n",
        "print(test_data_by_mols[\"mol_objs\"][0].GetNumAtoms())\n",
        "#draw mols:\n",
        "sample = train_data_by_mols.loc[5:8]\n",
        "grid = Draw.MolsToGridImage(sample[\"mol_objs\"],molsPerRow = 2,subImgSize=(400,400))\n",
        "#grid\n",
        "\n"
      ],
      "metadata": {
        "id": "YLVeoTZXjwQg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0eb1525-97c7-4a0a-8c37-47b81d5a7cdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# functions for setting graph molecule features\n",
        "# every node is atom, every edge is bond\n",
        "\n",
        "# TODO:EXPLORE ATOM AND EDGE FEATURES THAT MIGHT INDICATE TOXICITY\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from rdkit import Chem\n",
        "import torch\n",
        "import torch_geometric\n",
        "\n",
        "def get_nodes_features(mol):\n",
        "  ''' gets molecule and returns all of it's atoms' features,  can vary from 9 features '''\n",
        "  features_of_all_nodes = []\n",
        "\n",
        "  for atom in mol.GetAtoms():\n",
        "    node_feats = []\n",
        "\n",
        "    node_feats.append(atom.GetAtomicNum())\n",
        "    node_feats.append(atom.GetChiralTag())\n",
        "    node_feats.append(atom.GetDegree())\n",
        "    node_feats.append(atom.GetFormalCharge())\n",
        "    node_feats.append(atom.GetHybridization())\n",
        "    node_feats.append(atom.GetIsAromatic())\n",
        "    node_feats.append(atom.GetTotalNumHs())\n",
        "    node_feats.append(atom.GetNumRadicalElectrons())\n",
        "    node_feats.append(atom.IsInRing())\n",
        "\n",
        "    features_of_all_nodes.append(node_feats)\n",
        "\n",
        "  features_of_all_nodes = np.asarray(features_of_all_nodes)\n",
        "  return torch.tensor(features_of_all_nodes, dtype=torch.float)\n",
        "\n",
        "\n",
        "def get_edges_features(mol):\n",
        "  ''' gets molecule and returns all of it's bonds' features,  can vary from 2 features '''\n",
        "  features_of_all_edges = []\n",
        "\n",
        "  for bond in mol.GetBonds():\n",
        "    edge_feats = []\n",
        "    edge_feats.append(bond.IsInRing())\n",
        "    edge_feats.append(bond.GetBondTypeAsDouble())\n",
        "    features_of_all_edges += [edge_feats, edge_feats] # twice because this is undirected graph\n",
        "  features_of_all_edges = np.asarray(features_of_all_edges)\n",
        "  return torch.tensor(features_of_all_edges, dtype=torch.float)\n",
        "\n",
        "def get_mat_edges(mol):\n",
        "  ''' gets molecule and returns list of all it's edges by atoms idx '''\n",
        "  edge_indices = []\n",
        "  for bond in mol.GetBonds():\n",
        "    idx_of_atom1 = bond.GetBeginAtomIdx()\n",
        "    idx_of_atom2 = bond.GetEndAtomIdx()\n",
        "    edge_indices += [[idx_of_atom1, idx_of_atom2], [idx_of_atom2, idx_of_atom1]]\n",
        "  edge_indices = torch.tensor(edge_indices,dtype=torch.int)\n",
        "  edge_indices = edge_indices.t().view(2, -1) #transpose and reshape\n",
        "  return edge_indices\n"
      ],
      "metadata": {
        "id": "yWvdOfW_2QBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get/create the custom dataset from google drive\n",
        "#creating custom dataset for the GNN which includes mol properties\n",
        "import pandas as pd\n",
        "from rdkit import Chem\n",
        "import torch\n",
        "import torch_geometric\n",
        "from torch_geometric.data import DataLoader,Dataset, Data\n",
        "import numpy as np\n",
        "import io\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "node_feats = None\n",
        "edges_idxs = None\n",
        "edge_feats = None\n",
        "mol_graph_data = None\n",
        "custom_train_data=[]\n",
        "custom_test_data=[]\n",
        "NUM_GRAPHS_PER_BATCH = 128\n",
        "\n",
        "#train_aug_gnn = AUGMENTED_CASE == \"none augmented\" and \"not_augmented\" or \"augmented\"\n",
        "#test_aug_gnn = AUGMENTED_CASE == \"none augmented\" and \"not_augmented\" or (AUGMENTED_CASE == \"only train augmented\" and \"not_augmented\" ) or \"augmented\"\n",
        "\n",
        "for idx in range(len(train_data_by_mols[\"mol_objs\"])):\n",
        "  mol_obj = train_data_by_mols[\"mol_objs\"][idx]\n",
        "\n",
        "  mol_graph_data = Data(x=get_nodes_features(mol_obj),\n",
        "                        edge_index=get_mat_edges(mol_obj),\n",
        "                        edge_attr=get_edges_features(mol_obj),\n",
        "                        y=torch.tensor(train_data[\"label\"][idx]),\n",
        "                        smiles=train_data[\"smiles\"][idx]\n",
        "                        )\n",
        "  custom_train_data.append(mol_graph_data)\n",
        "\n",
        "print(custom_train_data[0].x.shape[1]) #features size of each node\n",
        "for idx in range(len(test_data_by_mols[\"mol_objs\"])):\n",
        "  mol_obj = test_data_by_mols[\"mol_objs\"][idx]\n",
        "\n",
        "  mol_graph_data = Data(x=get_nodes_features(mol_obj),\n",
        "                        edge_index=get_mat_edges(mol_obj),\n",
        "                        edge_attr=get_edges_features(mol_obj),\n",
        "                        y=torch.tensor(test_data[\"label\"][idx]),\n",
        "                        smiles=test_data[\"smiles\"][idx]\n",
        "                        )\n",
        "  custom_test_data.append(mol_graph_data)\n",
        "\n",
        "custom_train_loader = DataLoader(custom_train_data,batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
        "custom_test_loader = DataLoader(custom_test_data,batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n"
      ],
      "metadata": {
        "id": "zp4GAPCyslgE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1023e13d-e4b7-492c-87e5-12aa2ff05ab5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#testing if same originated smiles have same molecule object\n",
        "# for idx1 in range(len(mol_objs_train)):\n",
        "#   mol1 = mol_objs_train[idx1]\n",
        "#   for idx2 in range(len(mol_objs_train)):\n",
        "#     mol2 = mol_objs_train[idx2]\n",
        "#     if mol1.HasSubstructMatch(mol2) and mol2.HasSubstructMatch(mol1): #smiles are same molecule\n",
        "#       print(\"idx1\",idx1)\n",
        "#       print(\"idx2\",idx2)\n",
        "#       print(custom_train_data[idx1])\n",
        "#       print(custom_train_data[idx2])\n",
        "#       print(mol2)\n",
        "#       # grid = Draw.MolsToGridImage([mol1,mol2],molsPerRow = 2,subImgSize=(400,400))\n",
        "#       # grid\n",
        "#       break\n",
        "\n",
        "print(custom_train_data[234].x)\n",
        "print(custom_train_data[150].x)\n",
        "# idx1 234\n",
        "# idx2 150\n",
        "# Data(x=[23, 9], edge_index=[2, 50], edge_attr=[50, 2], y=1, smiles='c1ccc2c(c1)N(C[C@H](C)C[NH+](C)C)c1c(ccc(C#N)c1)S2')\n",
        "# Data(x=[23, 9], edge_index=[2, 50], edge_attr=[50, 2], y=1, smiles='C[NH+](C[C@H](CN1c2ccccc2Sc2ccc(C#N)cc21)C)C')\n",
        "# #draw mols:\n",
        "\n",
        "# sample = train_data_by_mols.loc[5:8]\n",
        "grid = Draw.MolsToGridImage([mol_objs_train[234],mol_objs_train[150]],molsPerRow = 2,subImgSize=(400,400))\n",
        "grid"
      ],
      "metadata": {
        "id": "GvfR6Q50l_fW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating model (simple)\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Sequential, Linear, BatchNorm1d, ReLU\n",
        "from torch_geometric.nn import TransformerConv, GATConv, TopKPooling, BatchNorm\n",
        "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
        "from torch_geometric.nn.conv.x_conv import XConv\n",
        "#torch.manual_seed(42)\n",
        "\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, feature_size):\n",
        "        super(GNN, self).__init__()\n",
        "        num_classes = 2\n",
        "        # NOTE: lower emb size in augmented results in much faster training, for size 1024 until epoch 100 model predicts only 0\n",
        "        embedding_size =8 #TODO: adjust embeeding size and not by rule of thumb\n",
        "\n",
        "        # GNN layers\n",
        "        self.conv1 = GATConv(feature_size, embedding_size, heads=3,dropout=0.1)\n",
        "        self.head_transform1 = Linear(embedding_size*3, embedding_size)\n",
        "        self.pool1 = TopKPooling(embedding_size, ratio=0.8)\n",
        "        self.conv2 = GATConv(embedding_size, embedding_size, heads=3,dropout=0.1)\n",
        "        self.head_transform2 = Linear(embedding_size*3, embedding_size)\n",
        "        self.pool2 = TopKPooling(embedding_size, ratio=0.5)\n",
        "        self.conv3 = GATConv(embedding_size, embedding_size, heads=3,dropout=0.1)\n",
        "        self.head_transform3 = Linear(embedding_size*3, embedding_size)\n",
        "        self.pool3 = TopKPooling(embedding_size, ratio=0.2)\n",
        "\n",
        "        # Linear layers\n",
        "        self.linear1 = Linear(embedding_size*2, 1024)\n",
        "        self.linear2 = Linear(1024, num_classes)\n",
        "\n",
        "    def forward(self, x, edge_attr, edge_index, batch_index,epoch):\n",
        "        # First block\n",
        "        # if epoch %10 == 0:\n",
        "        #   print(f\"before conv1 {x[0]} \\n\")\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "\n",
        "        # if epoch %10 == 0:\n",
        "        #   print(f\"after conv1 {x} \\n\")\n",
        "        #   print(f\"x[0] {x[0]} \\n\")\n",
        "\n",
        "        x = self.head_transform1(x)\n",
        "\n",
        "        x, edge_index, edge_attr, batch_index, _, _ = self.pool1(x,\n",
        "                                                        edge_index,\n",
        "                                                        None,\n",
        "                                                        batch_index)\n",
        "        x1 = torch.cat([gmp(x, batch_index), gap(x, batch_index)], dim=1)\n",
        "\n",
        "        # Second block\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.head_transform2(x)\n",
        "        x, edge_index, edge_attr, batch_index, _, _ = self.pool2(x,\n",
        "                                                        edge_index,\n",
        "                                                        None,\n",
        "                                                        batch_index)\n",
        "        x2 = torch.cat([gmp(x, batch_index), gap(x, batch_index)], dim=1)\n",
        "\n",
        "        # Third block\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = self.head_transform3(x)\n",
        "        x, edge_index, edge_attr, batch_index, _, _ = self.pool3(x,\n",
        "                                                        edge_index,\n",
        "                                                        None,\n",
        "                                                        batch_index)\n",
        "        x3 = torch.cat([gmp(x, batch_index), gap(x, batch_index)], dim=1)\n",
        "\n",
        "        # Concat pooled vectors\n",
        "        x = x1 + x2 + x3\n",
        "        # listX=[]\n",
        "        # for i in x:\n",
        "        #   if any([(i == item).all() for item in listX]):\n",
        "        #     print(\"found \\n\")\n",
        "        #     print(i)\n",
        "        #   else:\n",
        "        #     listX.append(torch.sort(i))\n",
        "        #     print(sorted(i))\n",
        "        # Output block\n",
        "        x = self.linear1(x).relu()\n",
        "        # if epoch %10 == 0:\n",
        "        #   print(f\"linear1 {x} \\n\")\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.linear2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "UhZjjZC2aLyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.data import DataLoader\n",
        "from sklearn.metrics import confusion_matrix, f1_score, \\\n",
        "    accuracy_score, precision_score, recall_score, roc_auc_score,precision_recall_curve,auc\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "\n",
        "#%% Loading the model\n",
        "sh = torch.Tensor(custom_train_data[0].x.shape[1])\n",
        "sh.to(device)\n",
        "model = GNN(feature_size=custom_train_data[0].x.shape[1])\n",
        "model = model.to(device)\n",
        "\n",
        "#%% Loss and Optimizer\n",
        "weights = torch.tensor([1,1], dtype=torch.float32).to(device)\n",
        "loss_fn = torch.nn.CrossEntropyLoss(weight=weights)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "\n",
        "\n",
        "#%% Prepare training\n",
        "NUM_GRAPHS_PER_BATCH = 32\n",
        "\n",
        "def train(epoch):\n",
        "    # Enumerate over the data\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for _, batch in enumerate(tqdm(custom_train_loader)):\n",
        "        # Use GPU\n",
        "        batch = batch.to(device)\n",
        "        # Reset gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Passing the node features and the connection info\n",
        "        pred = model(batch.x.float(),\n",
        "                                batch.edge_attr.float(),\n",
        "                                batch.edge_index,\n",
        "                                batch.batch,\n",
        "                                epoch)\n",
        "        # Calculating the loss and gradients\n",
        "        loss = loss_fn(pred,batch.y)\n",
        "        loss.backward()\n",
        "        # Update using the gradients\n",
        "        optimizer.step()\n",
        "\n",
        "        all_preds.append(np.argmax(pred.cpu().detach().numpy(), axis=1))\n",
        "        all_labels.append(batch.y.cpu().detach().numpy())\n",
        "    all_preds = np.concatenate(all_preds).ravel()\n",
        "    all_labels = np.concatenate(all_labels).ravel()\n",
        "    calculate_metrics(all_preds, all_labels, epoch, \"train\")\n",
        "    return loss\n",
        "\n",
        "def test(epoch):\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for batch in custom_test_loader:\n",
        "        batch.to(device)\n",
        "        pred = model(batch.x.float(),\n",
        "                        batch.edge_attr.float(),\n",
        "                        batch.edge_index,\n",
        "                        batch.batch,epoch)\n",
        "        loss = loss_fn(pred, batch.y)\n",
        "        all_preds.append(np.argmax(pred.cpu().detach().numpy(), axis=1))\n",
        "        all_labels.append(batch.y.cpu().detach().numpy())\n",
        "\n",
        "    all_preds = np.concatenate(all_preds).ravel()\n",
        "    all_labels = np.concatenate(all_labels).ravel()\n",
        "    calculate_metrics(all_preds, all_labels, epoch, \"test\")\n",
        "    return loss\n",
        "\n",
        "\n",
        "def calculate_metrics(y_pred, y_true, epoch, type):\n",
        "  #TODO:MODEL PREDICTS ALL MOLS AS 1\n",
        "    # for pred in y_pred:\n",
        "    #   if pred ==0:\n",
        "    #     print(\"predicteddddddddddddd 0 \")\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true,y_pred).ravel()\n",
        "    print(f\"tn: {tn} \\n \")\n",
        "    print(f\"fp: {fp} \\n \")\n",
        "    print(f\"fn: {fn} \\n \")\n",
        "    print(f\"tp: {tp} \\n \")\n",
        "    print(f\"\\n Confusion matrix: \\n {confusion_matrix(y_true,y_pred)}\")\n",
        "    print(f\"F1 Score: {f1_score(y_true,y_pred )}\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_true, y_pred)}\")\n",
        "    print(f\"Precision: {precision_score(y_true,y_pred)}\")\n",
        "    print(f\"Recall: {recall_score(y_true, y_pred)}\")\n",
        "    try:\n",
        "        roc = roc_auc_score(y_true, y_pred)\n",
        "        print(f\"ROC AUC: {roc}\")\n",
        "        precision, recall, _ = precision_recall_curve(y_true,  y_pred)\n",
        "        pr_auc = auc(recall, precision)\n",
        "        print(f\"PR AUC: {pr_auc}\")\n",
        "    except:\n",
        "        print(f\"ROC AUC: notdefined\")\n",
        "\n",
        "# %% Run the training\n",
        "def run_all_train():\n",
        "    for epoch in range(200):\n",
        "        # Training\n",
        "        model.train()\n",
        "        loss = train(epoch=epoch)\n",
        "        loss = loss.detach().cpu().numpy()\n",
        "        print(f\"Epoch {epoch} | Train Loss {loss}\")\n",
        "        # Testing\n",
        "        model.eval()\n",
        "        if epoch % 5 == 0:\n",
        "            loss = test(epoch=epoch)\n",
        "            loss = loss.detach().cpu().numpy()\n",
        "            print(f\"Epoch {epoch} | Test Loss {loss}\")\n",
        "        scheduler.step()\n",
        "    print(\"Done.\")\n",
        "\n",
        "run_all_train()\n"
      ],
      "metadata": {
        "id": "i0tnXAePc4Of",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "outputId": "cba25df7-18ae-4ea6-8b65-c1dbdf93e0c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/32 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor(-1.9453, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.9860, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.9209, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.7685, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.6665, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.1691, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.1365, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0737, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0621, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.2056, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.2364, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.4529, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.5287, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.7982, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(1.6788, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(2.3759, device='cuda:0', grad_fn=<UnbindBackward0>)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-5f9610bb307f>\u001b[0m in \u001b[0;36m<cell line: 118>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m \u001b[0mrun_all_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-40-5f9610bb307f>\u001b[0m in \u001b[0;36mrun_all_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch} | Train Loss {loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-5f9610bb307f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Passing the node features and the connection info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         pred = model(batch.x.float(),\n\u001b[0m\u001b[1;32m     41\u001b[0m                                 \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_attr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                                 \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-b39b1b385d6d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_attr, edge_index, batch_index, epoch)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mlistX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m           \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"found \\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-b39b1b385d6d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mlistX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m           \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"found \\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'bool' object has no attribute 'all'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating model (complicated, TODO:TEST IT)\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear, BatchNorm1d, ModuleList\n",
        "from torch_geometric.nn import TransformerConv, TopKPooling\n",
        "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
        "torch.manual_seed(42)\n",
        "\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, feature_size, model_params):\n",
        "        super(GNN, self).__init__()\n",
        "        embedding_size = model_params[\"model_embedding_size\"]\n",
        "        n_heads = model_params[\"model_attention_heads\"]\n",
        "        self.n_layers = model_params[\"model_layers\"]\n",
        "        dropout_rate = model_params[\"model_dropout_rate\"]\n",
        "        top_k_ratio = model_params[\"model_top_k_ratio\"]\n",
        "        self.top_k_every_n = model_params[\"model_top_k_every_n\"]\n",
        "        dense_neurons = model_params[\"model_dense_neurons\"]\n",
        "        edge_dim = model_params[\"model_edge_dim\"]\n",
        "\n",
        "        self.conv_layers = ModuleList([])\n",
        "        self.transf_layers = ModuleList([])\n",
        "        self.pooling_layers = ModuleList([])\n",
        "        self.bn_layers = ModuleList([])\n",
        "\n",
        "        # Transformation layer\n",
        "        self.conv1 = TransformerConv(feature_size,\n",
        "                                    embedding_size,\n",
        "                                    heads=n_heads,\n",
        "                                    dropout=dropout_rate,\n",
        "                                    edge_dim=edge_dim,\n",
        "                                    beta=True)\n",
        "\n",
        "        self.transf1 = Linear(embedding_size*n_heads, embedding_size)\n",
        "        self.bn1 = BatchNorm1d(embedding_size)\n",
        "\n",
        "        # Other layers\n",
        "        for i in range(self.n_layers):\n",
        "            self.conv_layers.append(TransformerConv(embedding_size,\n",
        "                                                    embedding_size,\n",
        "                                                    heads=n_heads,\n",
        "                                                    dropout=dropout_rate,\n",
        "                                                    edge_dim=edge_dim,\n",
        "                                                    beta=True))\n",
        "\n",
        "            self.transf_layers.append(Linear(embedding_size*n_heads, embedding_size))\n",
        "            self.bn_layers.append(BatchNorm1d(embedding_size))\n",
        "            if i % self.top_k_every_n == 0:\n",
        "                self.pooling_layers.append(TopKPooling(embedding_size, ratio=top_k_ratio))\n",
        "\n",
        "\n",
        "        # Linear layers\n",
        "        self.linear1 = Linear(embedding_size*2, dense_neurons)\n",
        "        self.linear2 = Linear(dense_neurons, int(dense_neurons/2))\n",
        "        self.linear3 = Linear(int(dense_neurons/2), 1)\n",
        "\n",
        "    def forward(self, x, edge_attr, edge_index, batch_index):\n",
        "        # Initial transformation\n",
        "        x = self.conv1(x, edge_index, edge_attr)\n",
        "        x = torch.relu(self.transf1(x))\n",
        "        x = self.bn1(x)\n",
        "\n",
        "        # Holds the intermediate graph representations\n",
        "        global_representation = []\n",
        "\n",
        "        for i in range(self.n_layers):\n",
        "            x = self.conv_layers[i](x, edge_index, edge_attr)\n",
        "            x = torch.relu(self.transf_layers[i](x))\n",
        "            x = self.bn_layers[i](x)\n",
        "            # Always aggregate last layer\n",
        "            if i % self.top_k_every_n == 0 or i == self.n_layers:\n",
        "                x , edge_index, edge_attr, batch_index, _, _ = self.pooling_layers[int(i/self.top_k_every_n)](\n",
        "                    x, edge_index, edge_attr, batch_index\n",
        "                    )\n",
        "                # Add current representation\n",
        "                global_representation.append(torch.cat([gmp(x, batch_index), gap(x, batch_index)], dim=1))\n",
        "\n",
        "        x = sum(global_representation)\n",
        "\n",
        "        # Output block\n",
        "        x = torch.relu(self.linear1(x))\n",
        "        x = F.dropout(x, p=0.8, training=self.training)\n",
        "        x = torch.relu(self.linear2(x))\n",
        "        x = F.dropout(x, p=0.8, training=self.training)\n",
        "        x = self.linear3(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "X69YLZiymlYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading model\n",
        "num_of_node_features = custom_train_data[0].x.shape[1]\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "params = {\n",
        "    \"model_edge_dim\":custom_train_data[0].edge_attr.shape[1],\n",
        "    \"batch_size\": 128,\n",
        "    \"learning_rate\": 0.01,\n",
        "    \"weight_decay\": 0.0001,\n",
        "    \"sgd_momentum\": 0.8,\n",
        "    \"scheduler_gamma\": 0.8,\n",
        "    \"pos_weight\": 1.3,\n",
        "    \"model_embedding_size\": 64,\n",
        "    \"model_attention_heads\": 3,\n",
        "    \"model_layers\": 4,\n",
        "    \"model_dropout_rate\": 0.2,\n",
        "    \"model_top_k_ratio\": 0.5,\n",
        "    \"model_top_k_every_n\": 1,\n",
        "    \"model_dense_neurons\": 256\n",
        "}\n",
        "\n",
        "model = GNN(feature_size = num_of_node_features,model_params=params)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "1cLwml3GoqlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating loss and optimizer"
      ],
      "metadata": {
        "id": "TIy1dSMKwi_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%% imports\n",
        "import torch\n",
        "from torch_geometric.data import DataLoader\n",
        "from sklearn.metrics import confusion_matrix, f1_score, \\\n",
        "    accuracy_score, precision_score, recall_score, roc_auc_score\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "def train_one_epoch(epoch, model, train_loader, optimizer, loss_fn):\n",
        "    # Enumerate over the data\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    running_loss = 0.0\n",
        "    step = 0\n",
        "    for _, batch in enumerate(tqdm(train_loader)):\n",
        "        # Use GPU\n",
        "        batch.to(device)\n",
        "        # Reset gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Passing the node features and the connection info\n",
        "        pred = model(batch.x.float(),\n",
        "                                batch.edge_attr.float(),\n",
        "                                batch.edge_index,\n",
        "                                batch.batch)\n",
        "        # Calculating the loss and gradients\n",
        "        loss = loss_fn(torch.squeeze(pred), batch.y.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # Update tracking\n",
        "        running_loss += loss.item()\n",
        "        step += 1\n",
        "        all_preds.append(np.rint(torch.sigmoid(pred).cpu().detach().numpy()))\n",
        "        all_labels.append(batch.y.cpu().detach().numpy())\n",
        "    all_preds = np.concatenate(all_preds).ravel()\n",
        "    all_labels = np.concatenate(all_labels).ravel()\n",
        "    calculate_metrics(all_preds, all_labels, epoch, \"train\")\n",
        "    return running_loss/step\n",
        "\n",
        "def test(epoch, model, test_loader, loss_fn):\n",
        "    all_preds = []\n",
        "    all_preds_raw = []\n",
        "    all_labels = []\n",
        "    running_loss = 0.0\n",
        "    step = 0\n",
        "    for batch in test_loader:\n",
        "        batch.to(device)\n",
        "        pred = model(batch.x.float(),\n",
        "                        batch.edge_attr.float(),\n",
        "                        batch.edge_index,\n",
        "                        batch.batch)\n",
        "        loss = loss_fn(torch.squeeze(pred), batch.y.float())\n",
        "\n",
        "         # Update tracking\n",
        "        running_loss += loss.item()\n",
        "        step += 1\n",
        "        all_preds.append(np.rint(torch.sigmoid(pred).cpu().detach().numpy()))\n",
        "        all_preds_raw.append(torch.sigmoid(pred).cpu().detach().numpy())\n",
        "        all_labels.append(batch.y.cpu().detach().numpy())\n",
        "\n",
        "    all_preds = np.concatenate(all_preds).ravel()\n",
        "    all_labels = np.concatenate(all_labels).ravel()\n",
        "    print(all_preds_raw[0][:10])\n",
        "    print(all_preds[:10])\n",
        "    print(all_labels[:10])\n",
        "    calculate_metrics(all_preds, all_labels, epoch, \"test\")\n",
        "    log_conf_matrix(all_preds, all_labels, epoch)\n",
        "    return running_loss/step\n",
        "\n",
        "def log_conf_matrix(y_pred, y_true, epoch):\n",
        "    # Log confusion matrix as image\n",
        "    cm = confusion_matrix(y_pred, y_true)\n",
        "    classes = [\"0\", \"1\"]\n",
        "    df_cfm = pd.DataFrame(cm, index = classes, columns = classes)\n",
        "    plt.figure(figsize = (10,7))\n",
        "    cfm_plot = sns.heatmap(df_cfm, annot=True, cmap='Blues', fmt='g')\n",
        "    cfm_plot.figure.savefig(f'data/images/cm_{epoch}.png')\n",
        "\n",
        "\n",
        "def calculate_metrics(y_pred, y_true, epoch, type):\n",
        "    print(f\"\\n Confusion matrix: \\n {confusion_matrix(y_pred, y_true)}\")\n",
        "    print(f\"F1 Score: {f1_score(y_true, y_pred)}\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_true, y_pred)}\")\n",
        "    prec = precision_score(y_true, y_pred)\n",
        "    rec = recall_score(y_true, y_pred)\n",
        "    print(f\"Precision: {prec}\")\n",
        "    print(f\"Recall: {rec}\")\n",
        "\n",
        "    try:\n",
        "        roc = roc_auc_score(y_true, y_pred)\n",
        "        print(f\"ROC AUC: {roc}\")\n",
        "\n",
        "    except:\n",
        "      return\n",
        "\n",
        "\n",
        "\n",
        "# Prepare training\n",
        "weight = torch.tensor([params[\"pos_weight\"]], dtype=torch.float32).to(device)\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=weight)\n",
        "optimizer = torch.optim.SGD(model.parameters(),\n",
        "                                    lr=params[\"learning_rate\"],\n",
        "                                    momentum=params[\"sgd_momentum\"],\n",
        "                                    weight_decay=params[\"weight_decay\"])\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=params[\"scheduler_gamma\"])\n",
        "\n",
        "\n",
        "model.train()\n",
        "for epoch in range(10):\n",
        "  loss = train_one_epoch(epoch, model, custom_train_loader, optimizer, loss_fn)\n",
        "  print(f\"Epoch {epoch} | Train Loss {loss}\")\n"
      ],
      "metadata": {
        "id": "yKWJOhTyvyHO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
